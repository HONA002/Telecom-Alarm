{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import datetime\n",
    "import time\n",
    "import os\n",
    "from shutil import rmtree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, requests, json, logging, pyotp, time, datetime, pandas\n",
    "from io import StringIO\n",
    "import tempfile, os, base64\n",
    "\n",
    "logging.captureWarnings(True)\n",
    "##  credential settings, this accont uses OTP, hence the otpsecret is provided\n",
    "\n",
    "username = \"\"  ##  You must enter your unique credentials in order to have access to local database\n",
    "password = \"\"\n",
    "secret = \"\"\n",
    "\n",
    "def get_new_token():\n",
    "    auth_tok = None\n",
    "    if os.path.isfile(TOKEN_PATH):\n",
    "        token_age = round(time.time() - os.path.getmtime(TOKEN_PATH),0)\n",
    "        seconds_since_midnight = (now - now.replace(hour=0, minute=0, second=0, microsecond=0)).total_seconds()\n",
    "        if token_age - (86400-seconds_since_midnight) < 0:\n",
    "            with open(TOKEN_PATH, 'r') as infile:\n",
    "                # You might consider a test API call to establish token validity here.\n",
    "                auth_tok = infile.read()\n",
    "    if not auth_tok:\n",
    "        auth_server_url = \"https://ios.mtnirancell.ir/api/auth/token/2fa/login/\"\n",
    "        verify_url = \"https://ios.mtnirancell.ir/api/auth/token/2fa/verify/\"\n",
    "        token_req_payload = {  \"password\": password,  \"username\": username}\n",
    "\n",
    "\n",
    "        token_response = requests.post(auth_server_url,data=token_req_payload, verify=False, allow_redirects=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "        if token_response.status_code !=200:\n",
    "            print(\"Failed to obtain token from the OAuth 2.0 server\", file=sys.stderr)\n",
    "            \n",
    "        else:\n",
    "            print(\"Successfuly authorized\")\n",
    "            totp = pyotp.TOTP(secret) \n",
    "            session_id = token_response.json()[\"session_id\"]\n",
    "            #print(totp.now() )\n",
    "\n",
    "            request = {  \"session_id\": session_id,  \"code\": totp.now()}\n",
    "            verify_response = requests.post(verify_url,data=request, verify=False, allow_redirects=False)   \n",
    "            #print(verify_response.json())   \n",
    "            auth_tok = verify_response.json()['auth_token']\n",
    "            with open(TOKEN_PATH, 'w') as outfile:\n",
    "                outfile.write(auth_tok)\n",
    "    return auth_tok\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SFP Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = pandas.read_excel('./SFP and Sync.xlsx',sheet_name='SFP ')\n",
    "AT = pandas.read_excel('./Sites_510.xlsx') #importing list of alarms and other files relating to telecom sites locations\n",
    "AT = AT[['Site','Name']]\n",
    "File_M = File.merge(AT,left_on='Site Name',right_on='Site',how='left')\n",
    "Max = File_M['First Occurred On'].max()\n",
    "Min = File_M['First Occurred On'].min()\n",
    "SY = Min.year\n",
    "SM = Min.month\n",
    "SD = Min.day\n",
    "EY = Max.year\n",
    "EM = Max.month\n",
    "ED = (Max.day) + 1\n",
    "Site_List = File_M['Name'].drop_duplicates()\n",
    "S = Site_List.array\n",
    "L = S.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly authorized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "t_dir = tempfile.TemporaryDirectory(dir = './')\n",
    "TOKEN_PATH = os.path.join(\n",
    "    t_dir.name, \n",
    "    'ios.token',\n",
    ")\n",
    "##\n",
    "##    function to obtain a new OAuth 2.0 token from the authentication server\n",
    "##\n",
    "\n",
    "## \n",
    "## \tobtain a token before calling the API for the first time\n",
    "##\n",
    "token = get_new_token()\n",
    "## \n",
    "## \tget the list of KPI IDs based on Technology, Vendor and then filter the names\n",
    "##  the KPI name should be available in selected technology and vendor\n",
    "##\n",
    "technology = \"LMBB\"\n",
    "vendor = \"Nokia\"\n",
    "indicators_url = '' # the URL of the local database\n",
    "headers  = {'authorization':'Token '+str(token),'Content-Type': 'application/json'}\n",
    "indicators_request_payload =  {  \"technology\": technology,  \"vendor\": vendor}\n",
    "indicators_response_payload = requests.post(indicators_url,json=indicators_request_payload, verify=False, allow_redirects=False,headers=headers)\n",
    "\n",
    "df_indicators = pandas.DataFrame(indicators_response_payload.json())\n",
    "\n",
    "kpis = ['Cell_Availability_All','ERAB_Drop_Rate_Act_All_No_QCI5','Interference_PUSCH_Avg',\n",
    "        'Erlang_VoLTE','ERAB_Setup_Succ_Rate_Initial','Payload_PDCP_Total_GByte',\n",
    "        'HOSR_Inter_Freq_Out','HOSR_Intra_Freq_Out','Throughput_UE_QCI9_DL_kbps']\n",
    "\n",
    "indicators = df_indicators[(df_indicators.name.isin(kpis))].loc[:,\"id\"].values.tolist()\n",
    "\n",
    "\n",
    "sdate = datetime.datetime.timestamp(datetime.datetime(SY,SM,SD))*1000  # Importing data of the time period related to alarm list in order to minimize the load on local database\n",
    "edate = datetime.datetime.timestamp(datetime.datetime(EY,EM,ED))*1000\n",
    "## \n",
    "## \tget the kpi data for selected indicators\n",
    "##  the KPI name should be available in selected technology and vendor\n",
    "##\n",
    "\n",
    "entity_type = \"site\"\n",
    "element_list = L # Importing data of every site which is included in the alarm list\n",
    "\n",
    "kpi_explore_url = ''\n",
    "headers  = {'authorization':'Token '+token,'Content-Type': 'application/json'}\n",
    "kpi_request_payload = {\"entities\":None,\n",
    "                    \"filters\":[{\"custom\":False,\"custom_name\":None,\"element_list\":[\"R5\",\"R10\"],\"type\":\"region\"}],\n",
    "                    \"indicators\":indicators,\n",
    "                    \"layer\":None,\n",
    "                    \"technology\":technology,\n",
    "                    \"vendor\":vendor,\n",
    "                    \"start_date\":sdate,\n",
    "                    \"end_date\":edate,\n",
    "                    \"granularity\":\"h\",\n",
    "                    \"entity_filters\":[{\"type\":entity_type,\"element_list\":element_list,\"custom\":False,\"custom_name\":None}],\n",
    "                    \"output\":\"csv\"}\n",
    "kpi_response_payload = requests.post(kpi_explore_url,json=kpi_request_payload, verify=False, allow_redirects=False,headers=headers)\n",
    "\n",
    "my_string = StringIO(kpi_response_payload.text)\n",
    "Main_4G = pandas.read_csv(my_string)\n",
    "# if not df_KPI.empty:\n",
    "#     print(df_KPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_4G['time'] = Main_4G['time'].astype('datetime64[ns]')\n",
    "File_M['First Occurred On'] = File_M['First Occurred On'].dt.round('H')\n",
    "File_M['Post'] = File_M['First Occurred On'] + pd.Timedelta(hours=2)\n",
    "File_M['Pre'] = File_M['First Occurred On'] - pd.Timedelta(hours=2)\n",
    "File_M['Post_U'] = File_M['Post'] + pd.Timedelta(hours=10)\n",
    "File_M['Pre_U'] = File_M['Pre'] - pd.Timedelta(hours=10)\n",
    "Set = Main_4G.merge(File_M, left_on=['time','element'], right_on=['First Occurred On','Name'], how='left')\n",
    "X = Set['element'].unique().tolist()\n",
    "Y = Set['Alarm Name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_F = pandas.DataFrame()\n",
    "PRE_F = pandas.DataFrame()\n",
    "CURRENT_F = pandas.DataFrame()\n",
    "for i in range(len(X)):\n",
    "    Na = Set[(Set['element'] == X[i]) & (Set['Alarm Name'].isna())]\n",
    "    for j in range(len(Y)):\n",
    "        Alarm = Set[(Set['element'] == X[i]) & (Set['Alarm Name'] == Y[j])]\n",
    "        Alarm = pandas.concat([Alarm,Na])\n",
    "        Alarm = Alarm.reset_index()\n",
    "        CUPR = Alarm['Pre'][0]\n",
    "        Alarm['Pre'] = Alarm['Pre'].fillna(CUPR)\n",
    "        CUPO = Alarm['Post'][0]\n",
    "        Alarm['Post'] = Alarm['Post'].fillna(CUPO)\n",
    "        FOO = Alarm['First Occurred On'][0]\n",
    "        Alarm['First Occurred On'] = Alarm['First Occurred On'].fillna(FOO)\n",
    "        ALNM = Alarm['Alarm Name'][0]\n",
    "        Alarm['Alarm Name'] = Alarm['Alarm Name'].fillna(ALNM)\n",
    "        ULPR = Alarm['Pre_U'][0]\n",
    "        Alarm['Pre_U'] = Alarm['Pre_U'].fillna(ULPR)\n",
    "        ULPO = Alarm['Post_U'][0]\n",
    "        Alarm['Post_U'] = Alarm['Post_U'].fillna(ULPO)\n",
    "        PRE = Alarm[(Alarm['time'] >= Alarm['Pre_U']) & (Alarm['time'] < Alarm['Pre'])]\n",
    "        POST = Alarm[(Alarm['time'] > Alarm['Post']) & (Alarm['time'] <= Alarm['Post_U'])]\n",
    "        CURRENT = Alarm[(Alarm['time'] >= Alarm['Pre']) & (Alarm['time'] <= Alarm['Post'])]\n",
    "        \n",
    "        POST = POST[['element','Alarm Name','First Occurred On','Payload_PDCP_Total_GByte','Throughput_UE_QCI9_DL_kbps','Interference_PUSCH_Avg','ERAB_Setup_Succ_Rate_Initial','Erlang_VoLTE','HOSR_Inter_Freq_Out',\n",
    "             'HOSR_Intra_Freq_Out','ERAB_Drop_Rate_Act_All_No_QCI5','Cell_Availability_All']].groupby(['element','Alarm Name','First Occurred On']).mean().reset_index()\n",
    "        PRE = PRE[['element','Alarm Name','First Occurred On','Payload_PDCP_Total_GByte','Throughput_UE_QCI9_DL_kbps','Interference_PUSCH_Avg','ERAB_Setup_Succ_Rate_Initial','Erlang_VoLTE','HOSR_Inter_Freq_Out',\n",
    "             'HOSR_Intra_Freq_Out','ERAB_Drop_Rate_Act_All_No_QCI5','Cell_Availability_All']].groupby(['element','Alarm Name','First Occurred On']).mean().reset_index()\n",
    "        CURRENT = CURRENT[['element','Alarm Name','First Occurred On','Payload_PDCP_Total_GByte','Throughput_UE_QCI9_DL_kbps','Interference_PUSCH_Avg','ERAB_Setup_Succ_Rate_Initial','Erlang_VoLTE','HOSR_Inter_Freq_Out',\n",
    "             'HOSR_Intra_Freq_Out','ERAB_Drop_Rate_Act_All_No_QCI5','Cell_Availability_All']].groupby(['element','Alarm Name','First Occurred On']).mean().reset_index()    \n",
    "            \n",
    "        POST_F = pandas.concat([POST_F,POST])\n",
    "        PRE_F = pandas.concat([PRE_F,PRE])  \n",
    "        CURRENT_F = pandas.concat([CURRENT_F,CURRENT]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = PRE_F.merge(POST_F,on=['element','Alarm Name','First Occurred On'],suffixes=('_PRE','_POST'),how='outer').merge(CURRENT_F,on=['element','Alarm Name','First Occurred On'],suffixes=('','_CURRENT'),how='outer')\n",
    "Final.to_csv('./Input/SFP.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sync Alarm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "File = pandas.read_excel('./SFP and Sync.xlsx',sheet_name='Sync ')\n",
    "File_M = File.merge(AT,left_on='Site Name',right_on='Site',how='left')\n",
    "Max = File_M['First Occurred On'].max()\n",
    "Min = File_M['First Occurred On'].min()\n",
    "SY = Min.year\n",
    "SM = Min.month\n",
    "SD = Min.day\n",
    "EY = Max.year\n",
    "EM = Max.month\n",
    "ED = (Max.day) + 1\n",
    "Site_List = File_M['Name'].drop_duplicates()\n",
    "S = Site_List.array\n",
    "L = S.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfuly authorized\n"
     ]
    }
   ],
   "source": [
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "t_dir = tempfile.TemporaryDirectory(dir = './')\n",
    "TOKEN_PATH = os.path.join(\n",
    "    t_dir.name, \n",
    "    'ios.token',\n",
    ")\n",
    "##\n",
    "##    function to obtain a new OAuth 2.0 token from the authentication server\n",
    "##\n",
    "\n",
    "## \n",
    "## \tobtain a token before calling the API for the first time\n",
    "##\n",
    "token = get_new_token()\n",
    "## \n",
    "## \tget the list of KPI IDs based on Technology, Vendor and then filter the names\n",
    "##  the KPI name should be available in selected technology and vendor\n",
    "##\n",
    "technology = \"LMBB\"\n",
    "vendor = \"Nokia\"\n",
    "indicators_url = ''\n",
    "headers  = {'authorization':'Token '+str(token),'Content-Type': 'application/json'}\n",
    "indicators_request_payload =  {  \"technology\": technology,  \"vendor\": vendor}\n",
    "indicators_response_payload = requests.post(indicators_url,json=indicators_request_payload, verify=False, allow_redirects=False,headers=headers)\n",
    "\n",
    "df_indicators = pandas.DataFrame(indicators_response_payload.json())\n",
    "\n",
    "kpis = ['Cell_Availability_All','ERAB_Drop_Rate_Act_All_No_QCI5','Interference_PUSCH_Avg',\n",
    "        'Erlang_VoLTE','ERAB_Setup_Succ_Rate_Initial','Payload_PDCP_Total_GByte',\n",
    "        'HOSR_Inter_Freq_Out','HOSR_Intra_Freq_Out','Throughput_UE_QCI9_DL_kbps']\n",
    "\n",
    "indicators = df_indicators[(df_indicators.name.isin(kpis))].loc[:,\"id\"].values.tolist()\n",
    "\n",
    "# duration = 7\n",
    "# d = datetime.timedelta(days = duration)\n",
    "# tod = datetime.datetime.now().replace(hour=00, minute=00,second=00,microsecond=00)\n",
    "# sdate = datetime.datetime.timestamp(tod - d)*1000   #1679689800000\n",
    "# edate = datetime.datetime.timestamp(tod)*1000       #1679985000000\n",
    "\n",
    "sdate = datetime.datetime.timestamp(datetime.datetime(SY,SM,SD))*1000\n",
    "edate = datetime.datetime.timestamp(datetime.datetime(EY,EM,ED))*1000\n",
    "## \n",
    "## \tget the kpi data for selected indicators\n",
    "##  the KPI name should be available in selected technology and vendor\n",
    "##\n",
    "\n",
    "entity_type = \"site\"\n",
    "element_list = L\n",
    "\n",
    "kpi_explore_url = ''\n",
    "headers  = {'authorization':'Token '+token,'Content-Type': 'application/json'}\n",
    "kpi_request_payload = {\"entities\":None,\n",
    "                    \"filters\":[{\"custom\":False,\"custom_name\":None,\"element_list\":[\"R5\",\"R10\"],\"type\":\"region\"}],\n",
    "                    \"indicators\":indicators,\n",
    "                    \"layer\":None,\n",
    "                    \"technology\":technology,\n",
    "                    \"vendor\":vendor,\n",
    "                    \"start_date\":sdate,\n",
    "                    \"end_date\":edate,\n",
    "                    \"granularity\":\"h\",\n",
    "                    \"entity_filters\":[{\"type\":entity_type,\"element_list\":element_list,\"custom\":False,\"custom_name\":None}],\n",
    "                    \"output\":\"csv\"}\n",
    "kpi_response_payload = requests.post(kpi_explore_url,json=kpi_request_payload, verify=False, allow_redirects=False,headers=headers)\n",
    "\n",
    "my_string = StringIO(kpi_response_payload.text)\n",
    "Main_4G = pandas.read_csv(my_string)\n",
    "# if not df_KPI.empty:\n",
    "#     print(df_KPI)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "Main_4G['time'] = Main_4G['time'].astype('datetime64[ns]')\n",
    "File_M['First Occurred On'] = File_M['First Occurred On'].dt.round('H')\n",
    "File_M['Post'] = File_M['First Occurred On'] + pd.Timedelta(hours=2)\n",
    "File_M['Pre'] = File_M['First Occurred On'] - pd.Timedelta(hours=2)\n",
    "File_M['Post_U'] = File_M['Post'] + pd.Timedelta(hours=10)\n",
    "File_M['Pre_U'] = File_M['Pre'] - pd.Timedelta(hours=10)\n",
    "Set = Main_4G.merge(File_M, left_on=['time','element'], right_on=['First Occurred On','Name'], how='left')\n",
    "X = Set['element'].unique().tolist()\n",
    "Y = Set['Alarm Name'].unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_F = pandas.DataFrame()\n",
    "PRE_F = pandas.DataFrame()\n",
    "CURRENT_F = pandas.DataFrame()\n",
    "for i in range(len(X)):\n",
    "    Na = Set[(Set['element'] == X[i]) & (Set['Alarm Name'].isna())]\n",
    "    for j in range(len(Y)):\n",
    "        Alarm = Set[(Set['element'] == X[i]) & (Set['Alarm Name'] == Y[j])]\n",
    "        Alarm = pandas.concat([Alarm,Na])\n",
    "        Alarm = Alarm.reset_index()\n",
    "        CUPR = Alarm['Pre'][0]\n",
    "        Alarm['Pre'] = Alarm['Pre'].fillna(CUPR)\n",
    "        CUPO = Alarm['Post'][0]\n",
    "        Alarm['Post'] = Alarm['Post'].fillna(CUPO)\n",
    "        FOO = Alarm['First Occurred On'][0]\n",
    "        Alarm['First Occurred On'] = Alarm['First Occurred On'].fillna(FOO)\n",
    "        ALNM = Alarm['Alarm Name'][0]\n",
    "        Alarm['Alarm Name'] = Alarm['Alarm Name'].fillna(ALNM)\n",
    "        ULPR = Alarm['Pre_U'][0]\n",
    "        Alarm['Pre_U'] = Alarm['Pre_U'].fillna(ULPR)\n",
    "        ULPO = Alarm['Post_U'][0]\n",
    "        Alarm['Post_U'] = Alarm['Post_U'].fillna(ULPO)\n",
    "        PRE = Alarm[(Alarm['time'] >= Alarm['Pre_U']) & (Alarm['time'] < Alarm['Pre'])]\n",
    "        POST = Alarm[(Alarm['time'] > Alarm['Post']) & (Alarm['time'] <= Alarm['Post_U'])]\n",
    "        CURRENT = Alarm[(Alarm['time'] >= Alarm['Pre']) & (Alarm['time'] <= Alarm['Post'])]\n",
    "        \n",
    "        POST = POST[['element','Alarm Name','First Occurred On','Payload_PDCP_Total_GByte','Throughput_UE_QCI9_DL_kbps','Interference_PUSCH_Avg','ERAB_Setup_Succ_Rate_Initial','Erlang_VoLTE','HOSR_Inter_Freq_Out',\n",
    "             'HOSR_Intra_Freq_Out','ERAB_Drop_Rate_Act_All_No_QCI5','Cell_Availability_All']].groupby(['element','Alarm Name','First Occurred On']).mean().reset_index()\n",
    "        PRE = PRE[['element','Alarm Name','First Occurred On','Payload_PDCP_Total_GByte','Throughput_UE_QCI9_DL_kbps','Interference_PUSCH_Avg','ERAB_Setup_Succ_Rate_Initial','Erlang_VoLTE','HOSR_Inter_Freq_Out',\n",
    "             'HOSR_Intra_Freq_Out','ERAB_Drop_Rate_Act_All_No_QCI5','Cell_Availability_All']].groupby(['element','Alarm Name','First Occurred On']).mean().reset_index()\n",
    "        CURRENT = CURRENT[['element','Alarm Name','First Occurred On','Payload_PDCP_Total_GByte','Throughput_UE_QCI9_DL_kbps','Interference_PUSCH_Avg','ERAB_Setup_Succ_Rate_Initial','Erlang_VoLTE','HOSR_Inter_Freq_Out',\n",
    "             'HOSR_Intra_Freq_Out','ERAB_Drop_Rate_Act_All_No_QCI5','Cell_Availability_All']].groupby(['element','Alarm Name','First Occurred On']).mean().reset_index()    \n",
    "            \n",
    "        POST_F = pandas.concat([POST_F,POST])\n",
    "        PRE_F = pandas.concat([PRE_F,PRE])  \n",
    "        CURRENT_F = pandas.concat([CURRENT_F,CURRENT]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final = PRE_F.merge(POST_F,on=['element','Alarm Name','First Occurred On'],suffixes=('_PRE','_POST'),how='outer').merge(CURRENT_F,on=['element','Alarm Name','First Occurred On'],suffixes=('','_CURRENT'),how='outer')\n",
    "Final.to_csv('./Input/Sync.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "path1 = glob.glob('./*tmp*')\n",
    "for file in path1:\n",
    "    rmtree(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
